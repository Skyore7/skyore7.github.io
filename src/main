# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

# python -m tensorboard.main --logdir=runs 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

from sklearn.model_selection import train_test_split
from sklearn.multioutput import MultiOutputRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from tqdm import tqdm
from models import RocketLeagueModel, RocketLeagueModel2, RocketLeagueModel3, RocketLeagueModel4, RocketLeagueModel6, RocketLeagueModel7

import h5py
import torch

import torch.nn.functional as F


from torch.utils.data import Dataset, DataLoader
from torch.utils.tensorboard import SummaryWriter
from torch import nn, optim

from data_preprocessing import preprocessTrain, preprocessTest

SUBMISSION_FILE = 'submission7_1'
MODEL_TEMPLATE = RocketLeagueModel7
MODEL_NAME = "model7_1"
EXPERIMENT_NAME = "my_experiment7_1"

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

path = "data/"

dtypes_traindf = pd.read_csv(path + "train_dtypes.csv")
dtypes_train = {k: (v if v != 'float16' else 'float32') for (k, v) in zip(dtypes_traindf.column, dtypes_traindf.dtype)}
dtypes_testdf = pd.read_csv(path + "test_dtypes.csv")
dtypes_test = {k: (v if v != 'float16' else 'float32') for (k, v) in zip(dtypes_testdf.column, dtypes_testdf.dtype)}

""" filenames_train = []
for i in range(10):
    filenames_train.append(preprocessTrain(path + "train_%i" % i)) """
#filename_test = preprocessTest(path + "test")

filenames_train = []
for i in range(10):
    filenames_train.append(path + "train_%i.h5" % i)

filename_test = path + "test.h5"

print(filenames_train)
print(filename_test)

train_tensors = []

for filename in filenames_train:
    with h5py.File(filename, "r") as f:
        # Access the 'data' dataset (the one where the preprocessed data is stored)
        train_data = f["data"][:]  # Use [:] to load the entire dataset into memory
        
        
        # Convert the data (which is a NumPy array) into a PyTorch tensor
        train_tensor = torch.tensor(train_data, dtype=torch.float32)
        print(train_tensor.shape)

        train_tensors.append(train_tensor)

train_tensor_all = torch.cat(train_tensors)
print(train_tensor_all.shape)

with h5py.File(filename_test, "r") as f:
    # Access the 'data' dataset (the one where the preprocessed data is stored)
    test_data = f["data"][:]  # Use [:] to load the entire dataset into memory
    
    
    # Convert the data (which is a NumPy array) into a PyTorch tensor
    test_tensor = torch.tensor(test_data, dtype=torch.float32)

# Now `data_tensor` is ready to be used in your model or for further processing
print(test_tensor.shape)
print(test_tensor)

class MyDataset(Dataset):
    def __init__(self, data_tensor, train=True):
        """
        Args:
            data_tensor (torch.Tensor): The tensor containing the dataset.
        """
        self.data_tensor = data_tensor
        self.train = train

    def __len__(self):
        return len(self.data_tensor)

    def __getitem__(self, idx):
        if self.train:
            features = self.data_tensor[idx, :-3]  # All but last 3 columns
            labels = self.data_tensor[idx, -3:]    # Last 3 columns
            return features, labels
        else:
            features = self.data_tensor[idx, :]   # All but first column (if needed)
            return features, torch.tensor([])

        

from sklearn.model_selection import train_test_split

from tqdm import tqdm

def train(train_tensor, model_path='%s.pth' % MODEL_NAME, patience=5, max_grad_norm=1.0):
    model = MODEL_TEMPLATE().to(device)
    #model.load_state_dict(torch.load(model_path))
    criterion = nn.CrossEntropyLoss()  # Example, adjust based on your task
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    # Set up TensorBoard writer
    writer = SummaryWriter(log_dir='runs/%s' % EXPERIMENT_NAME)

    # Split the train_tensor into training and validation sets (80% train, 20% validation)
    train_data, val_data = train_test_split(train_tensor, test_size=0.2, random_state=42)

    # Prepare your train and validation datasets and loaders
    train_dataset = MyDataset(train_data, True)
    val_dataset = MyDataset(val_data, True)  # Using True here as this will have labels

    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)

    # Training loop with early stopping
    epochs = 100  # Large number to allow early stopping to decide
    best_val_loss = float('inf')
    epochs_no_improve = 0

    for epoch in range(epochs):
        model.train()  # Set the model to training mode
        running_loss = 0.0

        # Use tqdm to show progress
        train_progress = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}", unit="batch")

        for inputs, labels in train_progress:
            inputs = inputs.to(device)
            labels = labels.to(device)

            # Zero the parameter gradients
            optimizer.zero_grad()

            # Forward pass
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()

            optimizer.step()

            # Track the loss
            running_loss += loss.item()

            # Update tqdm progress bar with the current loss
            train_progress.set_postfix({"Train Loss": loss.item()})

        train_progress.close()

        # Log training loss to TensorBoard
        avg_train_loss = running_loss / len(train_loader)
        writer.add_scalar('Loss/train', avg_train_loss, epoch)

        # Validation step
        model.eval()  # Set the model to evaluation mode
        val_loss = 0.0

        with torch.no_grad():  # No need to track gradients during validation
            val_progress = tqdm(val_loader, desc="Validation", unit="batch")
            for inputs, labels in val_progress:
                inputs = inputs.to(device)
                labels = labels.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                val_loss += loss.item()
                val_progress.set_postfix({"Validation Loss": loss.item()})
            val_progress.close()

        avg_val_loss = val_loss / len(val_loader)

        # Log validation loss to TensorBoard
        writer.add_scalar('Loss/validation', avg_val_loss, epoch)

        # Print progress
        print(f"Epoch {epoch+1}, "
              f"Train Loss: {avg_train_loss:.4f}, "
              f"Validation Loss: {avg_val_loss:.4f}")

        # Check for improvement
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            epochs_no_improve = 0
            torch.save(model.state_dict(), model_path)  # Save the best model
            print(f"Model saved with validation loss: {avg_val_loss:.4f}")
        else:
            epochs_no_improve += 1

        # Early stopping condition
        if epochs_no_improve >= patience:
            print(f"Stopping early after {patience} epochs with no improvement.")
            break

    # Close the writer when done
    writer.close()
def create_submission(test_tensor, model_path='%s.pth' % MODEL_NAME, device='cuda'):
    # Load the model
    model = MODEL_TEMPLATE().to(device)
    model.load_state_dict(torch.load(model_path))
    model.eval()  # Set the model to evaluation mode

    test_ids = test_tensor[:,:1].squeeze().to(torch.int)
    test_data = test_tensor[:,1:]

    # Prepare the test dataset and loader
    test_dataset = MyDataset(test_data, False)  # False means no labels for test data
    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)

    # Initialize lists to store the predictions
    team_A_predictions = []
    team_B_predictions = []

    # Make predictions on the test data
    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs = inputs.to(device)
            outputs = model(inputs)
            outputs = F.softmax(outputs, dim=1)

            # Discard the third output (assuming the model outputs 3 values per instance)
            team_A_scoring = outputs[:, 0].cpu().numpy()  # First output: team_A_scoring_within_10sec
            team_B_scoring = outputs[:, 1].cpu().numpy()  # Second output: team_B_scoring_within_10sec

            team_A_predictions.extend(team_A_scoring)
            team_B_predictions.extend(team_B_scoring)

    # Convert predictions to numpy arrays
    team_A_predictions = np.array(team_A_predictions)
    team_B_predictions = np.array(team_B_predictions)

    # Create a DataFrame with IDs and predictions
    submission_df = pd.DataFrame({
        'id': test_ids.numpy(),  # Renamed ID column
        'team_A_scoring_within_10sec': team_A_predictions.flatten(),
        'team_B_scoring_within_10sec': team_B_predictions.flatten()
    })

    # Save to CSV
    submission_df.to_csv('%s.csv' % SUBMISSION_FILE, index=False)
    print("Submission file saved as '%s.csv'." % SUBMISSION_FILE)


# Train with the updated train function
train(train_tensor_all)
create_submission(test_tensor)

